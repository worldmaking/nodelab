<!doctype html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora">
  <link rel="stylesheet" href="paper.css">
  <title>DIGM5520 Final Report</title>
</head>

<body>
  <article>
    <h1>WebXR Studio</h1>

    <!-- Add your names here - I was a bit shy to accidentally misspell or omit a name, so I think it's best if we each add our own. ;) -->

    <p class="authors">Douglas Gregory, Jorge de Oliveira, Kimberly Davis, Hrysovalanti Maheras</p>

    <p class="date">Prepared for York University course DIGM5520, 2021-12-04.</p>

    <!-- TODO: Add table of contents with anchor links to each section. -->

    <h2>Contents</h2>

    <ul>
      <li><a href="#overview">Overview</a>
        <ul>
          <li><a href="#related">Related Works</a></li>
        </ul>
      </li>
      <li><a href="#overview">Technical Components</a>
        <ul>
          <li><a href="#related">Replication</a>
            <ul>
              <li><a href="#websocket">WebSocket Protocol</a></li>
              <li><a href="#websocket">"Shared Scene" and Automerge Integration</a></li>
            </ul>
          </li>
          <li><a href="#ui">UI / HUD</a>
            <ul>
              <li><a href="#hud">HUD Panels</a></li>
              <li><a href="#emoji">Emojis</a></li>
              <li><a href="#print">Print in VR</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#worlds">Worlds</a>
        <ul>
          <li><a href="#reflect">Reflect</a></li>
        </ul>
      </li>
      <li><a href="#future">Future Work</a></li>
      <li><a href="#references">References</a></li>
    </ul>

    <h2 id="overview">Overview</h2>

    <h3 id="related">Related Works</h3>


    <h2 id="technical">Technical Components</h2>

    <h3 id="replication">Replication</h3>

    <p>
      Multi-user interaction occurs in one of an arbitrary number of "<strong>rooms</strong>", each one identified by a
      string key. Users who have connected to the same room are able to see and hear one another, and collaboratively
      edit a shared 3D scene that persists within that room, independent of all others.
    </p>
    <p>
      At present, we use one "room" per HTML entry point to the application (so each "world" discussed below has its own
      URL that always directs users into the same room). But this could be exposed to visitors to allow them to spin up
      semi-private instances of a given world, behind a room key known only to them and those they invite to join them.
    </p>
    <p>The application uses a client-server architecture over WebSocket connections to synchronize information about
      users
      in the shared environment of a given room, and the editable geometry of that shared environment. These two
      channels
      of information use different replication strategies:</p>

    <ul>
      <li>
        <p><strong>User data</strong> like avatar position and pose are transmitted eagerly from client to server
          30 times per second, and the server in turn broadcasts the set of last known poses to all clients
          in the room at a similar rate. These poses are then interpolated for smoother animated display.</p>
        <p>This data is one-way and ephemeral. A client may report changes only to their own user data, and
          they are considered authoritative over it. Only the latest snapshot of the data is retained in memory,
          and is cleared when a user disconnects.
        </p>
      </li>
      <li>
        <p><strong>Shared scene data</strong> like the geometry, materials, and transformations of 3D objects in the
          world,
          are maintained in a syncrhonized document through the use of the
          <a href="https://github.com/automerge/automerge" title="Automerge GitHub Repository">Automerge</a> library.
        </p>
        <p>This handles merging concurrent changes made to the scene by multiple users, and resolving conflicts
          so that all users eventually agree on the same state of the shared 3D scene. It also retains a full
          history of the changes made, so that the scene can be restored to previous versions or the edit sequence
          re-played.
        </p>
      </li>
    </ul>

    <p>A Node.js server is used as the central relay, as well as to host the user-facing web page content via the
      <a href="https://www.npmjs.com/package/express" title="express npm package">express</a> library.
    </p>

    <h4 id="websocket">WebSocket Communication Protocol</h4>

    <p>Because WebSockets are built on top of TCP, our communication is reliable and in-order, so we don't need to
      manually handle dropped or out-of-order messages. Interruptions in communication can still happen, but it's safe
      to
      treat such cases as a disconnection, and allow the user to re-connect from scratch, rather than trying to recover
      any state.</p>

    <p>All WebSocket communication between the client and server occurs via "Message" objects in JSON text format, with
      the following structure:</p>

    <pre>
{
  cmd: "command", // A short string indicating the message type.
  val: {
    // A JSON object with properties defining the mesage contents / payload.
  }
}</pre>

    <p>This communication structure is defined in a shared <a
        href="https://github.com/worldmaking/nodelab/blob/main/public/networkMessages.js">networkMessages.js</a> file
      used
      by both client and server code, for consistency.</p>

    <p>The initial connection flow works like this (see the client side in
      <a href="https://github.com/worldmaking/nodelab/blob/main/public/connect.mjs">connect.mjs</a>,
      and the server side in <a href="https://github.com/worldmaking/nodelab/blob/main/server.js">server.js</a>)
    </p>
    <ol>
      <li>
        <p>The client initiates a connection to the server, by appending the desired room key onto the end of the
          server's
          domain
          and using this as the WebSocket request URL.</p>
      </li>
      <li>
        <p>Upon receiving a new connection request, the server checks for an existing "room" object corresponding to the
          path component of the request URL (trailing or doubled slashes in the room key are ignored). If no existing
          room
          is found, it creates one, as an object dontaining:</p>
        <ul>
          <li>The room's key ("name")</li>
          <li>A map of all clients in the room</li>
          <li>A default scene document (see below), and a flag indicating whether there are local changes to syncrhonize
            to the clients (initially false)</li>
        </ul>
      </li>
      <li>
        <p>Next, the server assigns the newly-connected client a unique ID (using the <a
            href="https://www.npmjs.com/package/uuid" title="uuid npm package">uuid</a> library)
          in RFC4122 version 4 format (though we strip out the hyphens for compatibility as an Automerge actor ID). It
          then adds to the room's client map an object representing this client, containing:</p>
        <ul>
          <li>The WebSocket connection to use for communicating with this client</li>
          <li>A reference to the room object this client occupies</li>
          <li>A shared data structure, representing data to be synchronized to other clients in the room, further
            divided
            into two parts:
            <ul>
              <li>
                <p>A "<strong>volatile</strong>" component, containing information that changes rapidly and should be
                  syncrhonized eagerly, such as head and controller poses (initially assumed to be at the origin by
                  default). The client's unique ID is also stored here,
                  even though it does not change, for the convenience of being able to send an array of all the
                  "volatile"
                  structures and have them already pre-labelled with the client to whom they belong.
                </p>
              </li>
              <li>
                <p>A "<strong>user</strong>" component, containing long-lived user information, such as their display
                  name
                  and avatar customization. These are relayed only when a new user joins,
                  though in future they could also be updated when a user changes their display properties. (The current
                  interface does not give users a means to do this, but this is only a UI restriction)
                </p>
              </li>
            </ul>
          </li>
        </ul>
        <p>The server replies back to the client with a "handshake" message, containing the client's assigned unique ID,
          the server's own unique ID,
          and an array of the "shared" data objects for all other clients connected to the same room.</p>
      </li>
      <li>
        <p>
          As soon as the connection is established, the client sends a "user" message, containing the contents of its
          shared user data structure as described above.
        </p>
      </li>
      <li>
        <p>Upon receiving the "handshake" message, the client stores its unique ID and the server's, and uses these to
          set
          up its synchronized Automerge document (see below).
          It also generates "replicas" or local copies of each remote client's avatars.</p>
      </li>
      <li>
        <p>From then on, 30 times per second, the client sends the contents of its "volatile" shared data structure to
          the
          serve as part of a "pose" message.
          The server receives this data and uses it to overwrite the corresponding volatile portion of its data
          structure
          for the user.</p>
        <p>Similarly, 30 times per second, the server iterates through all of its rooms, and broadcasts an array of all
          volatile data structures for all users in each room to all of those users.</p>
      </li>
      <li>
        <p>When the server receives a "user" message, it repeats it to all other clients in that room, along with the
          unique ID of the client who sent it so they know to which replica the change should be applied.</p>
      </li>
      <li>
        <p>When the server detects that a client has disconnected, it sends an "exit" message to all clients in that
          room,
          containing the disconnected client's ID, so that they can delete the corresponding replica.</p>
      </li>
    </ol>

    <p>The networking code here is deliberately agnostic about the contents of the "volatile" and "user" data structures
      -
      simply copying them verbatim as opaque containers.
      This makes it easy for new prototypes and artworks to add new synchronized data into the protocol, without
      depending
      on changes to the networking infrastructure.
      Different experiences using divergent data streams can co-exist in different rooms running on the same shared
      server. An example of this in action are the "emotes"
      used in the multiplayer2.html demo, discussed below, where information about a user's current emoting state is
      injected into the volatile structure to replicate to other users.
    </p>

    <h4 id="automerge">"Shared Scene" and Automerge Integration</h4>

    <p>When a new room is created, the server creates a blank Automerge document with its unique ID,
      and tracks a synchronization state for that document for each user who connects to the room.
      As with the volatile and user data structures, this document is then treated as opaque: the
      server has no opinions about its contents.
    </p>
    <p>When a new user connects, immediately following the initial "handshake" message, the server generates an
      Automerge
      SyncMessage to send to that user, as a message with the "sync" command. Both client and server, upon receiving a
      "sync" message, apply its updates to their local copy of the shared document, and immediately try to generate a
      reply to send back as a "sync" message of their own. This conversation continues until both ends are up-to-date
      with
      one another and agree on the shared document contents (at which point Automerge will generate a null SyncMessage
      signifying that there is nothing new to discuss).
    </p>

    <p>This foundation of document and conversation state management is handled by the "Merger" class in <a
        href="https://github.com/worldmaking/nodelab/blob/develop/public/merge.js">merge.js</a>, which provides a
      wrapper
      around the common Automerge document transactions used on both client and server:
    </p>
    <ul>
      <li>
        <p><strong>merger = new Merger(sourceDocument, actorID)</strong> - creates a new Automerge document based on the
          provided source object (or initializes a blank document if this is omitted - that's what is used in the
          current
          version),
          using the provided actor ID string to identify the local author for change tracking. If this actor ID is
          omitted, Automerge will automatically generate one, but in our case we use the client's unique ID assigned by
          the server
          so that our document change tracking matches our IDs used for room membership.</p>
      </li>
      <li>
        <p><strong>merger.addClient(remoteActorID)</strong> - initializes conversation-tracking with another actor
          contributing to the document. The client calls this with the server's unique ID to track its synchronization
          state relative
          to the server. The server calls this with the unique client ID each time a client joins a room, to manage the
          syncrhonizaton states of each client relative to its local document version.</p>
      </li>
      <li>
        <p><strong>merger.applyChange(commitMessage, doc => { /* change function */})</strong> - applies a change to the
          local copy of the document, with an associated message describing the change. The client calls this to process
          changes made by the local user to the shared scene. The shared document is only ever modified in the body of
          this change function - outside of this, it is treated as immutable.</p>
      </li>
      <li>
        <p><strong>merger.makeSyncMessage(receiverID)</strong> - attempts to create and return an Automerge SyncMessage
          for the next stage of conversation with the actor identified by the "receiverID" string. If we're already
          synchronized
          with that recipient, this function returns null.</p>
      </li>
      <li>
        <p><strong>merger.handleSyncMessage(syncMesage, senderID)</strong> - accepts a string representation of an
          Automerge SyncMessage received over the network from the specified actor, and applies it to the local
          document.
          It returns an AutoMerge Patch data structure describing the full set of changes that this message has made to
          the document, which we can parse to update our front end.</p>
      </li>
      <li>
        <p><strong>merger.getDocument()</strong> - retrieves the current state of the shared Automerge document, which
          should be treated as read-only.</p>
      </li>
    </ul>

    <p>On the server side, when a SyncMessage is received from the client, we mark the room as potentially in need
      of syncrhonizaton. On our next 30-times-per-second loop sharing volatile information with all clients, we also
      try to generate a SyncMessage for each one, and send it if it is non-null (ie. if there's a change on the server
      that the remote client does not yet know about).
    </p>

    <p>On the client side, we create an abstraction over this basic document-synchronization, tailored to the needs of
      synchronizing a THREE.js scene, specifically. This is provided by the "SharedScene" class in
      <a href="https://github.com/worldmaking/nodelab/blob/develop/public/sharedScene.mjs">sharedScene.mjs</a>. This
      class
      maintains collections of THREE.Geometry, Material, and Object3D objects used in the local copy of the scene, and
      associates with each one a unique ID matching them to their corresponding entry in the Automerge document. This ID
      is stored in the objects' "userData" object as a "mergeID" field. This allows code that interfaces with the shared
      scene to work with regular THREE.js object references as they would for conventional/non-shared 3D content, rather
      than working via a specialized synchronization handle or other abstraction. It also maintains maps of these
      mergeIDs
      back to their corresponding objects, so that they can be fetched and modified in response to remote changes.
    </p>

    <p>Code that modifies the shared scene can create new geometries, materials, and meshes as THREE.js code normally
      does. To publish a new mesh, it just needs to call shared.registerMesh(mesh), and the SharedScene class handles
      making the necessary changes to the Automerge document and marking the object with its assigned unique ID.
      It automatically handles registering the mesh's geometry and materials too, though this can also be done manually
      with shared.registerGeometry() and .registerMaterial() respectively.
    </p>

    <p>Similarly, when changing an object's transformation or parenting, it suffices to call
      shared.updateTransformation(obj) or shared.placeInParent(obj, parentObjectOrNull) for the SharedScene to
      make the corresponding document updates.</p>

    <p>The SharedScene class does not perform any de-duplication if two pieces of code (local or remote) attempt to
      create equivalent geometry/material assets. But it does allow code to search its internal maps using eg.
      shared.sceneGeometry.getByName(geoName), to check for and re-use an already-registered shared asset rather
      than constructing a new one. It also does not currently handle reference counting or deletion of geometry and
      mesh objects that are no longer in use &mdash; this is an area for future improvement.
    </p>

    <p>Upon connection, the client constructs a new shared scene, which internally creates a THREE.Scene root object
      (initially with no unique ID assigned) to add to the current world, and relays any sync messages received from the
      server through it by calling shared.handleSyncMessage(syncMessage, serverID).
    </p>

    <p>Once the initial exchange of SyncMessages with the server reaches its end (merger.makeSyncMessage(serverID)
      returns
      null), then we know we're up to date with the present state of the shared scene as stored on the server. At this
      point, the SharedScene class examines the document to check whether it's been initialized. If not, it takes
      responsibility for initializing it by creating three Automerge tables:
    </p>
    <ul>
      <li>
        <p><strong>doc.geometries</strong> - contains a row for each THREE.BufferGeometry used by objects in the shared
          scene, with columns for the geometry's name, position attributes, and index buffer. (This bears expanding to
          include normal and texture coordinate attributes).</p>
      </li>
      <li>
        <p><strong>doc.materials</strong> - contains a row for each THREE.Material used by objects in the shared scene,
          with columsn for the material's name and RGB color triplet. (Materials are currently assumed to always be
          MeshLambertMaterial type, but this could be expanded to include more general materials)</p>
      </li>
      <li>
        <p><strong>doc.objects</strong> - contains a row for the scene root object, and a row for each THREE.Object3D
          object in the shared scene, with columns for their name, parent ID, position, rotation quaternion, scale, and
          (for mesh objects) their geometry and material IDs</p>
      </li>
    </ul>
    <p>It then proceeds to add a "root" entry into the objects table, with nulls for all elements but its name, setting
      it
      apart from any objects to be added later. It takes the unique key assigned to this row when adding it to the table
      and applies it to the scene root object, so that objects referencing</p>

    <p>Each time a SyncMessage is received from the server and applied to the local copy of the document, we parse the
      resulting Automerge Patch object for...
    </p>
    <ul>
      <li>
        <p>New rows in the <strong>geometries</strong> table - which we instantiate as new BufferGeometry instances with
          that row's unique key as their mergeID.</p>
      </li>
      <li>
        <p>New rows in the <strong>materials</strong> table - which we instantiate as new MeshLambertMaterial instances
          with that row's unique key as their mergeID.</p>
      </li>
      <li>
        <p>Changes to the <strong>objects</strong> table - which could include...</p>
        <ul>
          <li>
            <p>Empty changes, corresponding to a deleted object which we remove from our scene graph.</p>
          </li>
          <li>
            <p>Changes to rows whose key does <em>not</em> exist in our sceneObjects map, which correspond to new
              objects
              to add to
              the scene. If the object has a null position, we know it must be the scene root, and assign this row's key
              to
              our scene root object's mergeID. Otherwise, we assume it to be a mesh, and instantiate a THREE.Mesh with
              the
              data stored in the row.</p>
          </li>
          <li>
            <p>Changes to rows whose key <em>does</em> exist in our sceneObjects map, which correspond to modified
              objects. We parse the change for modifications to apply to the object's position, quaternion, scale, or
              parent.</p>
          </li>
        </ul>
      </li>
    </ul>
    <p>Changes are processed in this order, so that by the time we encounter a mesh that might reference the ID of a
      geometry or material, they have already been added to our maps. Parenting is handled last, to ensure we have the
      parent in our map before we try to add children to it.</p>

    <p>This processing of changes is the most complex aspect of the solution, and it requires further expansion to cover
      the full generality of content that could be authored in a THREE.js scene. But it offers a major advantage over
      destroying the scene and re-building it from scratch by deserializing JSON as we'd considered earlier in the
      project
      (beyond the performance gains from not doing that): it means that other code can hold references to objects in the
      shared scene from frame to frame and use them like any other THREE.js object, without risk that those old
      references
      will get invalidated and need to be refreshed following a remote update.
    </p>

    <p>The SharedScene class also exposes onSceneObjectAdded/Removed callbacks so that outside code can get
      notifications
      when either local or remote changes have created or destroyed objects in the scene.
    </p>

    <p>Inside the application's animation loop, once each frame, the SharedScene is asked if it has any new local
      changes
      to share with the server (via shared.tryGenerateSyncMessage()), and if so, a message is sent. This limits the
      amount of traffic and processing required in the event that many scripts make changes in rapid succession, such
      as in response to multiple mouse events in a single frame. This structure is easy to throttle further should
      performace problems arise in future.
    </p>


    <h3 id="ui">UI / HUD</h3>

    <p> The UI / HUD (Heads-Up Display) was built with accessbility and ease to use in mind. The buttons on the HUD
      panels were made to be
      big
      with bright colours so to indicate clearly what is being selected. An emoji system was introduced to give more
      alternatives to the player to express themselves in a way that can be easily understood and seen by others in the
      world. Then finally a print function was added that can print text in the wourld as a way to help debug the VR
      version with ease.</p>
    <!-- TODO: more can be added here -->

    <h4 id="hud">HUD Panels</h4>

    <p> The panels componet of the project uses the same in world 3D presence represented by 3D panels that can be
      interacted either by mouse click, or in VR by using the controller. This was achieved by using a raycast object
      that
      can be casted either from the mouse position, in case the client is using keyboard and mouse, or from the
      controller
      position, in case the client is using VR.</p>
    <!-- TODO: Add more to this section, I did not work on it so I don't know what else to add -->

    <h4 id="emoji">Emojis</h4>
    <p>The emoji system was built to give the player more ways to express how they feel while navigating in the world.
      This was achieved by spawning in 3D emojis that would animate on top of the player to signify their current
      emotion
      towards the world at that moment.
    </p>
    <p>The first step into spawning the emoji is to get a parent component as soon as the application is launched, the
      parent component serves as an anchor point where the emoji will always follow. Fot this parent component we chose
      to
      use the body representation of the player in the application world, that way everyone in the world will always
      know
      who used the emoji.
    </p>
    <p>The next step would come when the player decided to emote. As soon as they trigger the button a method is called
      where we get a time stamp of the time when the method was called, this is meant to be used as a way for the
      program
      to know
      when it is suitable to delete the emoji, the current time is 5 seconds after it has been triggered.
    </p>
    <p>Once this is done we then set the position of the emoji and also set the parent to the body object. Then to make
      sure the code is not expensive to run we call another method that actually spawns the emoji. This method does one
      of
      two functions, it either loads the emoji from a file path, or if that emoji has been previously used it recycles
      the
      emoji from a cached model.
    </p>

    <h4 id="print">Print in VR</h4>
    <p>The print in VR method was done using some of the basic principles of the emoji method. It gets a parent object
      that we can then
      parent the 3D text to it so it can anchor itself to that object in the world.
      Every string sent to the print method is added to an array, this array has a capacity of ten elements, if an 11th
      element is added
      the fisrt element in that array is then deleted and all the other elements shifts one position. The capacity of
      ten
      elements was chosen
      so it can be easy for the user to read the messages coming through without being overwhelmed by many messages at
      once.
    </p>

    <h2 id="worlds">Worlds</h2>

    <h3 id="reflect">Reflect</h3>
    <video width="600" poster="media/reflect.png" controls>
      <source src="media/ReflectVideo.mp4" type="video/mp4">
      <source src="media/ReflectVideo.mp4" type="video/ogg">
      Your browser does not support HTML video.
    </video>
    <h4>Abstract</h4>
    <p>Reflect is a Virtual Reality artwork that also works on the web. It is a virtual representation of the physical
      world that places users inside a gallery space. This space encapsulates an assemblage of point lights that float
      around and fluctuate as chemicals that react with each other. We combined two mechanisms stemming from two
      different biological species into the behavior of the lights. The result is a system of participatory lights
      adaptive to other participants' positionality and other world objects. Reflect is a place for internet users to
      visit, move inside it, interact with its lighting system, and together with other users can apply changes to the
      colors of the world. This project encompasses a multi-user experience inside a dynamic environment and follows a
      naturalistic perspective in the design of its interactions.</p>

    <h4>Overview - Concepts and Motivation</h4>

    <p>The motivations behind the design of Reflect's interaction system derive from inspiration found in nature's
      biological systems. Our project worked with two biological systems: a firefly's lighting mechanism, known as
      bioluminescence, and a chameleon's color adaptation mechanism. Like how a firefly produces light by a chemical
      reaction inside its body[Strogatz, 2015, p.37], each light in our system changes its intensity or luminosity based
      on a chemical reaction system algorithm. The programming for the changes in luminance relies on a set of reaction
      rules found in a prey-predator system of chemicals. They derive from the Lotka-Volterra differential equation
      [Campuzano, 2021]: </p>
      <center><img src="media/math-1.png" style="width:100px;height:40px;"></center>
    <ul>
      <li> x and y are the populations of the different chemicals;</li>
      <li>x' and y' show how the two populations grow with respect to their individual rates;</li>
      <li>The constants a, b, c,d, explain how the different chemicals interact;</li>
    </ul>
    <p>The color adaptability of the lighting system emulates a chameleon changing the color of its skin to that of its
      surroundings [Duffy, 2003]. </p>

    <p>Taking into consideration the work done by our peers, we wanted to use the choices made by each participant,
      i.e., the color choice on the homepage, to create endless reactions within the space. Participants enter the space
      and are greeted by a system of floating pointlights. Each light pulsates with other lights. As the participants
      move and explore the space, the lights start to react by changing their color to match the colors chosen by each
      participant when nearby. The color change causes a chain reaction as their colors spread throughout the space. As
      the lights move away from the visitors, their colors sync with their neighboring lights, creating a unique
      environment based on the choices and interactions made by each visitor. Another layer was the integration of the
      voice chat to emphasize the shared atmosphere that this environment creates.</p>

    <h4>Challenges and Solutions</h4>
    <p>During the making of this project, we faced some challenges that influenced both our technical and aesthetic
      decisions. One of the main challenges was implementing our ideas into a primary virtual world composed of standard
      lighting, a floor, and an avatar's body controlling its movements. Due to the nature of our project that has a
      custom lighting system and a custom avatar, we had to figure out a solution that did not affect the utilization of
      these standard presets for other teams. We worked together with our peers, responsible for setting that
      introductory scene together for our ideas of the avatar's body and our lighting system. We came up with the
      modularization of the avatar's characteristics and the turning on and off of the lights. </p>

    <p>Another significant challenge we had was that we could not test our work in a virtual reality setting since VR
      equipment was not easily accessible. When equipped with VR glasses, we managed to test our system in a VR setting,
      and we found out that the performance of our system was slightly slower than the performance of the system in the
      WebXR environment. Since the library we were using to create our content, namely, THREE.js, could not afford the
      creation of more than thirty lights per scene [threejs.org] for seamless interaction with our system, we had to
      decrease the number of lights in the space.</p>

    <p>In addition to the performance issues we had with working with 3D objects in a virtual environment, we had to
      test some of our ideas in a 2D system first before we moved everything into a three-dimensional scene. In
      designing the chemical reaction between the light objects, we began visualizing the system using a custom shader
      that stretched the scale of our light objects to give the impression of the lights being three-dimensional objects
      when in reality, they were two-dimensional [threejs.org]. However, for the interaction of our lights in a 3D
      environment, this approach did not work well because we could not see our lights from different angles. We ended
      up turning our 2D lights into 3D to work with two lists of objects. One was internal and included a smaller number
      of objects that carried the reaction rules for the chemical behavior of our system. The other was external and
      included a more significant number of objects and the programming of our three-dimensional geometries. We had to
      work with two lists of objects with their positions and utilize the smaller list of objects to control some of the
      behavioral characteristics, which were the scale of our lights and the maximum range of the light intensities,
      of the more extensive list of 3D objects.</p>

    <p>The final challenge we had to overcome while dealing with the design and implementation of this shared space was
      to create a unified experience for every participant. Initially, we were testing with one participant in the space
      to figure out the avatar's movements and apply our color-sharing interaction between the avatar and the nearby
      lights. What we managed to accomplish was to pass along this interaction to all users who share this space.
      Finally, we were able to track the movements of all users interacting and offer a slightly different experience
      for each participant based on the timestamp to which they joined the world. We found that to propagate changes
      between users, we needed to move the multiplayer interaction design from the client-side architecture to the
      server-side architecture of our application. On the server side, we can track the different entry points of each
      user and save whatever change has preceded. We are still working on a more permanent solution where remote users
      can share all changes in a unified experience.</p>

    <h4>A description of future work to be done on this project</h4>
    <p>Having a world where multiple users can have a unified experience would mean that Reflect can be a space for
      sculptors, painters, musicians, and digital artists in general to exhibit their artworks. We hope our current
      system will evolve into a sensory system that tracks other artworks or other artificial entities added to the
      world. An example of this evolution would be adding forces that attract and repel [Shiffman, 2012] our lights to
      and from a series of exhibited artworks, depending on the amount of lighting that has the greatest probability of
      complimenting each work.</p>
    <p>Apart from having artists exhibit their work in the Reflect world, we plan to refactor our lighting system into a
      module of lights [jetbrains] that can be part of complex worlds. We hope it can be helpful for contextualized
      pieces that engage with different modalities. The repurposing of our lighting system into a module we hope will
      allow us to work with musicians and sound artists that treat sound as energy transducing between the elements that
      make the core of its sound. In addition, we would very much like to see that energy exchanged between light and
      sound as it is transducing into a generative performance.</p>
    <p>While working on this project, we came across a competition that shared some of the conceptual aspects with the
      Reflect world, namely the annual architecture competition [architecturecompetition.com]. The theme for the year
      2022 is Museum of Emotions and focuses on the affective properties of two polarized spaces, which can evoke
      positive and negative emotions, respectively [architecturecompetition.com]. We wish to explore the behavior of the
      lighting between the two different spaces that influence each other and affect the atmosphere of each space,
      according to their contrasting lighting intensities. Our idea is to divide our lighting system in half and
      distribute it across the two spaces, one housing the prey and the second housing the predators. We believe that by
      dividing our Reflect world into two opposite spaces that interact with each other based on the dynamics found in
      the relations between the lights themselves, we will explore if there are any affective properties within our
      lighting system.</p>

    <h4>Conclusion</h4>

    <p>Our goal has been to create a collective experience between human beings and artificial life lights inside a
      gallery or a studio space. A shared space between living and non-living beings that co-create dynamically the
      virtual world surrounding them both. We strived to reimagine the natural world as an open possibility inside the
      virtual world. We believe that nature's examples can drive technological development. Since the technologies we
      are using today are not fully sustainable, we see a potential change in how energy is produced. Studying
      biological systems with the help of simulation tools makes it plausible to explore more sustainable life models,
      as in the example of the fireflies that produce light with small amounts of energy. Reflect is a speculative
      design for a more sustainable future.</p>




    <h2 id="future">Future Work</h2>
    <p></p>

    <h2 id="references">References</h2>
    <ul>
      <p>
        <li>Breeders, Bee. “Museum of Emotions / Architecture Competition.” Museum of Emotions. Accessed December 14,
          2021.
          <a
            href="https://architecturecompetitions.com/museumofemotions/">https://architecturecompetitions.com/museumofemotions/</a>.
        </li>
      </p>
      <p>
        <li>Campuzano, Juan Carlos Ponce. Lotka-Volterra model. Accessed December 14, 2021.
          <a
            href="https://teaching.smp.uq.edu.au/scims/Appl_analysis/Lotka_Volterra.html">https://teaching.smp.uq.edu.au/scims/Appl_analysis/Lotka_Volterra.html</a>.
        </li>
      </p>
      <p>
        <li>Duffy, B.R., G.M.P. O'Hare, A.N. Martin, J.F. Bradley, and B. Schon. “Agent Chameleons: Agent Minds and
          Bodies.”
          Proceedings 11th IEEE International Workshop on Program Comprehension, n.d.
          <a href="https://doi.org/10.1109/casa.2003.1199312">https://doi.org/10.1109/casa.2003.1199312</a>.
        </li>
      </p>
      <p>
        <li>“Refactoring Javascript: WebStorm.” WebStorm Help. Accessed December 14, 2021.
          <a
            href="https://www.jetbrains.com/help/webstorm/specific-javascript-refactorings.html#javascript_rename_function">https://www.jetbrains.com/help/webstorm/specific-javascript-refactorings.html#javascript_rename_function</a>.
        </li>
      </p>
      <p>
        <li>Shiffman, Daniel, Shannon Fry, and Zannah Marsh. “FORCES.” Essay. In The Nature of Code. Mountain View
          (California): s.n., 2012.</li>
      </p>
      <p>
        <li>Strogatz, Steven H. “Fireflies and the Inevitability of Sync.” Essay. In Sync: How Order Emerges from Chaos
          in
          the Universe, Nature, and Daily Life, 37. New York: Hachette Books, 2015.</li>
      </p>
      <p>
        <li>“Three.js.” three.js docs. Accessed December 14, 2021. <a
            href="https://threejs.org/docs/#api/en/lights/PointLight">https://threejs.org/docs/#api/en/lights/PointLight</a>.
        </li>
      </p>
      <p>
        <li>“Webgl Particles Waves Example.” Three.js webgl - particles - waves. Accessed December 14, 2021.
          <a
            href="https://threejs.org/examples/webgl_points_waves.html">https://threejs.org/examples/webgl_points_waves.html</a>.
        </li>
      </p>
    </ul>
  </article>
</body>

</html>