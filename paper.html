<!doctype html>

<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DIGM5520 Final Report</title>
</head>

<body>
  <h1>WebXR Studio</h1>

  <!-- Add your names here - I was a bit shy to accidentally misspell or omit a name, so I think it's best if we each add our own. ;) -->
  <p class="date">2021 - 12 - 04</p>
  <p class="authors">Douglas Gregory, Jorge de Oliveira</p>

  <!-- TODO: Add table of contents with anchor links to each section. -->

  <h2>Overview</h2>

  <h3>Related Works</h3>


  <h2>Technical Components</h2>

  <h3>Replication</h3>

  <p>
    Multi-user interaction occurs in one of an arbitrary number of "<strong>rooms</strong>", each one identified by a
    string key. Users who have connected
    to the same room are able to see and hear one another, and collaboratively edit a shared 3D scene that persists
    within that room,
    independent of all others.
  </p>
  <p>
    At present, we use one "room" per HTML entry point to the application (so each "world" discussed below has its own
    URL that always
    directs users into the same room). But this could be exposed to visitors to allow them to spin up semi-private
    instances of a given world,
    behind a room key known only to them and those they invite to join them.
  </p>
  <p>The application uses a client-server architecture over WebSocket connections to synchronize information about users
    in the shared environment of a given room, and the editable geometry of that shared environment. These two channels
    of information
    use different replication strategies:</p>

  <ul>
    <li>
      <p><strong>User data</strong> like avatar position and pose are transmitted eagerly from client to server
        30 times per second, and the server in turn broadcasts the set of last known poses to all clients
        in the room at a similar rate. These poses are then interpolated for smoother animated display.</p>
      <p>This data is one-way and ephemeral. A client may report changes only to their own user data, and
        they are considered authoritative over it. Only the latest snapshot of the data is retained in memory,
        and is cleared when a user disconnects.
      </p>
    </li>
    <li>
      <p><strong>Shared scene data</strong> like the geometry, materials, and transformations of 3D objects in the
        world,
        are maintained in a syncrhonized document through the use of the
        <a href="https://github.com/automerge/automerge" title="Automerge GitHub Repository">Automerge</a> library.
      </p>
      <p>This handles merging concurrent changes made to the scene by multiple users, and resolving conflicts
        so that all users eventually agree on the same state of the shared 3D scene. It also retains a full
        history of the changes made, so that the scene can be restored to previous versions or the edit sequence
        re-played.
      </p>
    </li>
  </ul>

  <p>A Node.js server is used as the central relay, as well as to host the user-facing web page content via the
    <a href="https://www.npmjs.com/package/express" title="express npm package">express</a> library.
  </p>

  <h4>WebSocket Communication Protocol</h4>

  <p>All WebSocket communication with the server transmits "Message" objects in JSON text format, with the following
    structure:</p>

  <pre>
  {
    cmd: "command", // A short string indicating the message type.
    val: {
      // A JSON object with properties defining the mesage contents / payload.
    }
  }
  </pre>

  <p>This communication structure is defined in a shared <a
      href="https://github.com/worldmaking/nodelab/blob/main/public/networkMessages.js">networkMessages.js</a> file used
    by both client and server code, for consistency.</p>

  <p>The initial connection flow works like this (see the client side in
    <a href="https://github.com/worldmaking/nodelab/blob/main/public/connect.mjs">connect.mjs</a>,
    and the server side in <a href="https://github.com/worldmaking/nodelab/blob/main/server.js">server.js</a>)
  </p>
  <ol>
    <li>
      <p>The client initiates a connection to the server, by appending the desired room key onto the end of the server's
        domain
        and using this as the WebSocket request URL.</p>
    </li>
    <li>
      <p>Upon receiving a new connection request, the server checks for an existing "room" object corresponding to the
        path component of the request URL (trailing or doubled slashes in the room key are ignored). If no existing room
        is found, it creates one, as an object dontaining:</p>
      <ul>
        <li>The room's key ("name")</li>
        <li>A map of all clients in the room</li>
        <li>A default scene document (see below), and a flag indicating whether there are local changes to syncrhonize
          to the clients (initially false)</li>
      </ul>
    </li>
    <li>
      <p>Next, the server assigns the newly-connected client a unique ID (using the <a
          href="https://www.npmjs.com/package/uuid" title="uuid npm package">uuid</a> library)
        in RFC4122 version 4 format (though we strip out the hyphens for compatibility as an Automerge actor ID). It
        then adds to the room's client map an object representing this client, containing:</p>
      <ul>
        <li>The WebSocket connection to use for communicating with this client</li>
        <li>A reference to the room object this client occupies</li>
        <li>A shared data structure, representing data to be synchronized to other clients in the room, further divided
          into two parts:
          <ul>
            <li>
              <p>A "<strong>volatile</strong>" component, containing information that changes rapidly and should be
                syncrhonized eagerly, such as head and controller poses (initially assumed to be at the origin by
                default). The client's unique ID is also stored here,
                even though it does not change, for the convenience of being able to send an array of all the "volatile"
                structures and have them already pre-labelled with the client to whom they belong.
              </p>
            </li>
            <li>
              <p>A "<strong>user</strong>" component, containing long-lived user information, such as their display name
                and avatar customization. These are relayed only when a new user joins,
                though in future they could also be updated when a user changes their display properties. (The current
                interface does not give users a means to do this, but this is only a UI restriction)
              </p>
            </li>
          </ul>
        </li>
      </ul>
      <p>The server replies back to the client with a "handshake" message, containing the client's assigned unique ID,
        the server's own unique ID,
        and an array of the "shared" data objects for all other clients connected to the same room.</p>
    </li>
    <li>
      <p>
        As soon as the connection is established, the client sends a "user" message, containing the contents of its
        shared user data structure as described above.
      </p>
    </li>
    <li>
      <p>Upon receiving the "handshake" message, the client stores its unique ID and the server's, and uses these to set
        up its synchronized Automerge document (see below).
        It also generates "replicas" or local copies of each remote client's avatars.</p>
    </li>
    <li>
      <p>From then on, 30 times per second, the client sends the contents of its "volatile" shared data structure to the
        serve as part of a "pose" message.
        The server receives this data and uses it to overwrite the corresponding volatile portion of its data structure
        for the user.</p>
      <p>Similarly, 30 times per second, the server iterates through all of its rooms, and broadcasts an array of all
        volatile data structures for all users in each room to all of those users.</p>
    </li>
    <li>
      <p>When the server receives a "user" message, it repeats it to all other clients in that room, along with the
        unique ID of the client who sent it so they know to which replica the change should be applied.</p>
    </li>
    <li>
      <p>When the server detects that a client has disconnected, it sends an "exit" message to all clients in that room,
        containing the disconnected client's ID, so that they can delete the corresponding replica.</p>
    </li>
  </ol>

  <p>The networking code here is deliberately agnostic about the contents of the "volatile" and "user" data structures -
    simply copying them verbatim as opaque containers.
    This makes it easy for new prototypes and artworks to add new synchronized data into the protocol, without depending
    on changes to the networking infrastructure.
    Different experiences using divergent data streams can co-exist in different rooms running on the same shared
    server. An example of this in action are the "emotes"
    used in the multiplayer2.html demo, discussed below, where information about a user's current emoting state is
    injected into the volatile structure to replicate to other users.
  </p>

  <h4>"Shared Scene" and Automerge Integration</h4>

  <p>When a new room is created, the server creates a blank Automerge document with its unique ID,
    and tracks a synchronization state for that document for each user who connects to the room.
    As with the volatile and user data structures, this document is then treated as opaque: the
    server has no opinions about its contents.
  </p>
  <p>When a new user connects, immediately following the initial "handshake" message, the server generates an Automerge
    SyncMessage to
    send to that user, as a message with the "sync" command. Both client and server, upon receiving a "sync" message,
    apply its updates
    to their local copy of the shared document, and immediately try to generate a reply to send back as a "sync" message
    of their own.
    This conversation continues until both ends are up-to-date with one another and agree on the shared document
    contents (at which
    point Automerge will generate a null SyncMessage signifying that there is nothing new to discuss).
  </p>

  <p>This foundation of document and conversation state management is handled by the "Merger" class in <a
      href="https://github.com/worldmaking/nodelab/blob/develop/public/merge.js">merge.js</a>, which provides a wrapper
    around the common Automerge document transactions used on both client and server:
  </p>
  <ul>
    <li>
      <p><strong>merger = new Merger(sourceDocument, actorID)</strong> - creates a new Automerge document based on the
        provided source object (or initializes a blank document if this is omitted - that's what is used in the current
        version),
        using the provided actor ID string to identify the local author for change tracking. If this actor ID is
        omitted, Automerge will automatically generate one, but in our case we use the client's unique ID assigned by
        the server
        so that our document change tracking matches our IDs used for room membership.</p>
    </li>
    <li>
      <p><strong>merger.addClient(remoteActorID)</strong> - initializes conversation-tracking with another actor
        contributing to the document. The client calls this with the server's unique ID to track its synchronization
        state relative
        to the server. The server calls this with the unique client ID each time a client joins a room, to manage the
        syncrhonizaton states of each client relative to its local document version.</p>
    </li>
    <li>
      <p><strong>merger.applyChange(commitMessage, doc => { /* change function */})</strong> - applies a change to the
        local copy of the document, with an associated message describing the change. The client calls this to process
        changes made by the local user to the shared scene. The shared document is only ever modified in the body of
        this change function - outside of this, it is treated as immutable.</p>
    </li>
    <li>
      <p><strong>merger.makeSyncMessage(receiverID)</strong> - attempts to create and return an Automerge SyncMessage
        for the next stage of conversation with the actor identified by the "receiverID" string. If we're already
        synchronized
        with that recipient, this function returns null.</p>
    </li>
    <li>
      <p><strong>merger.handleSyncMessage(syncMesage, senderID)</strong> - accepts a string representation of an
        Automerge SyncMessage received over the network from the specified actor, and applies it to the local document.
        It returns an AutoMerge Patch data structure describing the full set of changes that this message has made to
        the document, which we can parse to update our front end.</p>
    </li>
    <li>
      <p><strong>merger.getDocument()</strong> - retrieves the current state of the shared Automerge document, which
        should be treated as read-only.</p>
    </li>
  </ul>

  <p>On the client side, we create an abstraction over this basic document-synchronization, tailored to the needs of
    synchronizing a THREE.js scene, specifically. This is provided by the "SharedScene" class in
    <a href="https://github.com/worldmaking/nodelab/blob/develop/public/sharedScene.js">sharedScene.js</a>. Upon
    connection, the client constructs a new shared scene, which internally creates a THREE.Scene root object
    to add to the current world, and relays any sync messages received from the server through it.
  </p>


  <h3>UI/HUD</h3>

  <p> The UI/HUD was built with accessbility and ease to use in mind. The buttons on the HUD panels were made to be big
    with bright colours so to indicate clearly what is being selected. An emoji system was introduced to give more
    alternatives to the player to express themselves in a way that can be easily understood and seen by others in the
    world. Then finally a print function was added that can print text in the wourld as a way to help debug the VR
    version with ease.</p>
  <!-- TODO: more can be added here -->

  <h4>HUD Panels</h4>

  <p> The panels componet of the project uses the same in world 3D presence represented by 3D panels that can be
    interacted either by mouse click, or in VR by using the controller. This was achieved by using a raycast object that
    can be casted either from the mouse position, in case the client is using keyboard and mouse, or from the controller
    position, in case the client is using VR.</p>
  <!-- TODO: Add more to this section, I did not work on it so I don't know what else to add -->

  <h4>Emojis</h4>
  <p>The emoji system was built to give the player more ways to express how they feel while navigating in the world.
    This was achieved by spawning in 3D emojis that would animate on top of the player to signify their current emotion
    towards the world at that moment.
  </p>
  <p>The first step into spawning the emoji is to get a parent component as soon as the application is launched, the
    parent component serves as an anchor point where the emoji will always follow. Fot this parent component we chose to
    use the body representation of the player in the application world, that way everyone in the world will always know
    who used the emoji.
  </p>
  <p>The next step would come when the player decided to emote. As soon as they trigger the button a method is called
    where we get a time stamp of the time when the method was called, this is meant to be used as a way for the program
    to know
    when it is suitable to delete the emoji, the current time is 5 seconds after it has been triggered.
  </p>
  <p>Once this is done we then set the position of the emoji and also set the parent to the body object. Then to make
    sure the code is not expensive to run we call another method that actually spawns the emoji. This method does one of
    two functions, it either loads the emoji from a file path, or if that emoji has been previously used it recycles the
    emoji from a cached model.
  </p>

  <h4>Print in VR</h4>
  <p>The print in VR method was done using some of the basic principles of the emoji method. It gets a parent object
    that we can then
    parent the 3D text to it so it can anchor itself to that object in the world.
    Every string sent to the print method is added to an array, this array has a capacity of ten elements, if an 11th
    element is added
    the fisrt element in that array is then deleted and all the other elements shifts one position. The capacity of ten
    elements was chosen
    so it can be easy for the user to read the messages coming through without being overwhelmed by many messages at
    once.
  </p>

  <h2>Worlds</h2>

  <h2>Future Work</h2>
  <p></p>
</body>

</html>